{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "80xnUmoI7fBX"
   },
   "source": [
    "##### Copyright 2020 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "8nvTnfs6Q692"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WmfcMK5P5C1G"
   },
   "source": [
    "# Introduction to the TensorFlow Models NLP library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cH-oJ8R6AHMK"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/official_models/nlp/nlp_modeling_library_intro\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/models/blob/master/official/colab/nlp/nlp_modeling_library_intro.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/models/blob/master/official/colab/nlp/nlp_modeling_library_intro.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://storage.googleapis.com/tensorflow_docs/models/official/colab/nlp/nlp_modeling_library_intro.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0H_EFIhq4-MJ"
   },
   "source": [
    "## Learning objectives\n",
    "\n",
    "In this Colab notebook, you will learn how to build transformer-based models for common NLP tasks including pretraining, span labelling and classification using the building blocks from [NLP modeling library](https://github.com/tensorflow/models/tree/master/official/nlp/modeling)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2N97-dps_nUk"
   },
   "source": [
    "## Install and import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "459ygAVl_rg0"
   },
   "source": [
    "### Install the TensorFlow Model Garden pip package\n",
    "\n",
    "*  `tf-models-official` is the stable Model Garden package. Note that it may not include the latest changes in the `tensorflow_models` github repo. To include latest changes, you may install `tf-models-nightly`,\n",
    "which is the nightly Model Garden package created daily automatically.\n",
    "*  `pip` will install all models and dependencies automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y-qGkdh6_sZc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Apidwalin\\Anaconda3\\envs\\tf\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 437, in _error_catcher\n",
      "    yield\n",
      "  File \"C:\\Users\\Apidwalin\\Anaconda3\\envs\\tf\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 519, in read\n",
      "    data = self._fp.read(amt) if not fp_closed else b\"\"\n",
      "  File \"C:\\Users\\Apidwalin\\Anaconda3\\envs\\tf\\lib\\site-packages\\pip\\_vendor\\cachecontrol\\filewrapper.py\", line 62, in read\n",
      "    data = self.__fp.read(amt)\n",
      "  File \"C:\\Users\\Apidwalin\\Anaconda3\\envs\\tf\\lib\\http\\client.py\", line 457, in read\n",
      "    n = self.readinto(b)\n",
      "  File \"C:\\Users\\Apidwalin\\Anaconda3\\envs\\tf\\lib\\http\\client.py\", line 501, in readinto\n",
      "    n = self.fp.readinto(b)\n",
      "  File \"C:\\Users\\Apidwalin\\Anaconda3\\envs\\tf\\lib\\socket.py\", line 589, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"C:\\Users\\Apidwalin\\Anaconda3\\envs\\tf\\lib\\ssl.py\", line 1071, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"C:\\Users\\Apidwalin\\Anaconda3\\envs\\tf\\lib\\ssl.py\", line 929, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "socket.timeout: The read operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Apidwalin\\Anaconda3\\envs\\tf\\lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 216, in _main\n",
      "    status = self.run(options, args)\n",
      "  File \"C:\\Users\\Apidwalin\\Anaconda3\\envs\\tf\\lib\\site-packages\\pip\\_internal\\cli\\req_command.py\", line 182, in wrapper\n",
      "    return func(self, options, args)\n",
      "  File \"C:\\Users\\Apidwalin\\Anaconda3\\envs\\tf\\lib\\site-packages\\pip\\_internal\\commands\\install.py\", line 325, in run\n",
      "    reqs, check_supported_wheels=not options.target_dir\n",
      "  File \"C:\\Users\\Apidwalin\\Anaconda3\\envs\\tf\\lib\\site-packages\\pip\\_internal\\resolution\\legacy\\resolver.py\", line 183, in resolve\n",
      "    discovered_reqs.extend(self._resolve_one(requirement_set, req))\n",
      "  File \"C:\\Users\\Apidwalin\\Anaconda3\\envs\\tf\\lib\\site-packages\\pip\\_internal\\resolution\\legacy\\resolver.py\", line 388, in _resolve_one\n",
      "    abstract_dist = self._get_abstract_dist_for(req_to_install)\n",
      "  File \"C:\\Users\\Apidwalin\\Anaconda3\\envs\\tf\\lib\\site-packages\\pip\\_internal\\resolution\\legacy\\resolver.py\", line 340, in _get_abstract_dist_for\n",
      "    abstract_dist = self.preparer.prepare_linked_requirement(req)\n",
      "  File \"C:\\Users\\Apidwalin\\Anaconda3\\envs\\tf\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 469, in prepare_linked_requirement\n",
      "    hashes=self._get_linked_req_hashes(req)\n",
      "  File \"C:\\Users\\Apidwalin\\Anaconda3\\envs\\tf\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 259, in unpack_url\n",
      "    hashes=hashes,\n",
      "  File \"C:\\Users\\Apidwalin\\Anaconda3\\envs\\tf\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 130, in get_http_url\n",
      "    link, downloader, temp_dir.path, hashes\n",
      "  File \"C:\\Users\\Apidwalin\\Anaconda3\\envs\\tf\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 282, in _download_http_url\n",
      "    for chunk in download.chunks:\n",
      "  File \"C:\\Users\\Apidwalin\\Anaconda3\\envs\\tf\\lib\\site-packages\\pip\\_internal\\network\\utils.py\", line 88, in response_chunks\n",
      "    decode_content=False,\n",
      "  File \"C:\\Users\\Apidwalin\\Anaconda3\\envs\\tf\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 576, in stream\n",
      "    data = self.read(amt=amt, decode_content=decode_content)\n",
      "  File \"C:\\Users\\Apidwalin\\Anaconda3\\envs\\tf\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 541, in read\n",
      "    raise IncompleteRead(self._fp_bytes_read, self.length_remaining)\n",
      "  File \"C:\\Users\\Apidwalin\\Anaconda3\\envs\\tf\\lib\\contextlib.py\", line 130, in __exit__\n",
      "    self.gen.throw(type, value, traceback)\n",
      "  File \"C:\\Users\\Apidwalin\\Anaconda3\\envs\\tf\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 442, in _error_catcher\n",
      "    raise ReadTimeoutError(self._pool, None, \"Read timed out.\")\n",
      "pip._vendor.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out.\n"
     ]
    }
   ],
   "source": [
    "!pip install -q tf-models-official==2.3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e4huSSwyAG_5"
   },
   "source": [
    "### Import Tensorflow and other libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jqYXqtjBAJd9"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.python.keras.api._v1.keras.utils' has no attribute 'register_keras_serializable'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-a6abcff838b8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mofficial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlp\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodeling\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mofficial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodeling\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnetworks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\tensorflow1\\models\\official\\nlp\\modeling\\layers\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;34m\"\"\"Layers package definition.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m# pylint: disable=wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mofficial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodeling\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattention\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mofficial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodeling\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcls_head\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mofficial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodeling\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdense_einsum\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDenseEinsum\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\tensorflow1\\models\\official\\nlp\\modeling\\layers\\attention.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mofficial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodeling\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmasked_softmax\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[0mEinsumDense\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEinsumDense\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\tensorflow1\\models\\official\\nlp\\modeling\\layers\\masked_softmax.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[1;33m@\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister_keras_serializable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpackage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Text'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mMaskedSoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m   \"\"\"Performs a softmax with optional masking on a tensor.\n",
      "\u001b[1;32mc:\\users\\apidwalin\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\util\\module_wrapper.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    191\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m       \u001b[0mattr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfmw_wrapped_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfmw_public_apis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow.python.keras.api._v1.keras.utils' has no attribute 'register_keras_serializable'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from official.nlp import modeling\n",
    "from official.nlp.modeling import layers, losses, models, networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "djBQWjvy-60Y"
   },
   "source": [
    "## BERT pretraining model\n",
    "\n",
    "BERT ([Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)) introduced the method of pre-training language representations on a large text corpus and then using that model for downstream NLP tasks.\n",
    "\n",
    "In this section, we will learn how to build a model to pretrain BERT on the masked language modeling task and next sentence prediction task. For simplicity, we only show the minimum example and use dummy data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MKuHVlsCHmiq"
   },
   "source": [
    "### Build a `BertPretrainer` model wrapping `TransformerEncoder`\n",
    "\n",
    "The [TransformerEncoder](https://github.com/tensorflow/models/blob/master/official/nlp/modeling/networks/transformer_encoder.py) implements the Transformer-based encoder as described in [BERT paper](https://arxiv.org/abs/1810.04805). It includes the embedding lookups and transformer layers, but not the masked language model or classification task networks.\n",
    "\n",
    "The [BertPretrainer](https://github.com/tensorflow/models/blob/master/official/nlp/modeling/models/bert_pretrainer.py) allows a user to pass in a transformer stack, and instantiates the masked language model and classification networks that are used to create the training objectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EXkcXz-9BwB3"
   },
   "outputs": [],
   "source": [
    "# Build a small transformer network.\n",
    "vocab_size = 100\n",
    "sequence_length = 16\n",
    "network = modeling.networks.TransformerEncoder(\n",
    "    vocab_size=vocab_size, num_layers=2, sequence_length=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0NH5irV5KTMS"
   },
   "source": [
    "Inspecting the encoder, we see it contains few embedding layers, stacked `Transformer` layers and are connected to three input layers:\n",
    "\n",
    "`input_word_ids`, `input_type_ids` and `input_mask`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lZNoZkBrIoff"
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(network, show_shapes=True, dpi=48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o7eFOZXiIl-b"
   },
   "outputs": [],
   "source": [
    "# Create a BERT pretrainer with the created network.\n",
    "num_token_predictions = 8\n",
    "bert_pretrainer = modeling.models.BertPretrainer(\n",
    "    network, num_classes=2, num_token_predictions=num_token_predictions, output='predictions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d5h5HT7gNHx_"
   },
   "source": [
    "Inspecting the `bert_pretrainer`, we see it wraps the `encoder` with additional `MaskedLM` and `Classification` heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2tcNfm03IBF7"
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(bert_pretrainer, show_shapes=True, dpi=48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F2oHrXGUIS0M"
   },
   "outputs": [],
   "source": [
    "# We can feed some dummy data to get masked language model and sentence output.\n",
    "batch_size = 2\n",
    "word_id_data = np.random.randint(vocab_size, size=(batch_size, sequence_length))\n",
    "mask_data = np.random.randint(2, size=(batch_size, sequence_length))\n",
    "type_id_data = np.random.randint(2, size=(batch_size, sequence_length))\n",
    "masked_lm_positions_data = np.random.randint(2, size=(batch_size, num_token_predictions))\n",
    "\n",
    "outputs = bert_pretrainer(\n",
    "    [word_id_data, mask_data, type_id_data, masked_lm_positions_data])\n",
    "lm_output = outputs[\"masked_lm\"]\n",
    "sentence_output = outputs[\"classification\"]\n",
    "print(lm_output)\n",
    "print(sentence_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bnx3UCHniCS5"
   },
   "source": [
    "### Compute loss\n",
    "Next, we can use `lm_output` and `sentence_output` to compute `loss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k30H4Q86f52x"
   },
   "outputs": [],
   "source": [
    "masked_lm_ids_data = np.random.randint(vocab_size, size=(batch_size, num_token_predictions))\n",
    "masked_lm_weights_data = np.random.randint(2, size=(batch_size, num_token_predictions))\n",
    "next_sentence_labels_data = np.random.randint(2, size=(batch_size))\n",
    "\n",
    "mlm_loss = modeling.losses.weighted_sparse_categorical_crossentropy_loss(\n",
    "    labels=masked_lm_ids_data,\n",
    "    predictions=lm_output,\n",
    "    weights=masked_lm_weights_data)\n",
    "sentence_loss = modeling.losses.weighted_sparse_categorical_crossentropy_loss(\n",
    "    labels=next_sentence_labels_data,\n",
    "    predictions=sentence_output)\n",
    "loss = mlm_loss + sentence_loss\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wrmSs8GjHxVw"
   },
   "source": [
    "With the loss, you can optimize the model.\n",
    "After training, we can save the weights of TransformerEncoder for the downstream fine-tuning tasks. Please see [run_pretraining.py](https://github.com/tensorflow/models/blob/master/official/nlp/bert/run_pretraining.py) for the full example.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k8cQVFvBCV4s"
   },
   "source": [
    "## Span labeling model\n",
    "\n",
    "Span labeling is the task to assign labels to a span of the text, for example, label a span of text as the answer of a given question.\n",
    "\n",
    "In this section, we will learn how to build a span labeling model. Again, we use dummy data for simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xrLLEWpfknUW"
   },
   "source": [
    "### Build a BertSpanLabeler wrapping TransformerEncoder\n",
    "\n",
    "[BertSpanLabeler](https://github.com/tensorflow/models/blob/master/official/nlp/modeling/models/bert_span_labeler.py) implements a simple single-span start-end predictor (that is, a model that predicts two values: a start token index and an end token index), suitable for SQuAD-style tasks.\n",
    "\n",
    "Note that `BertSpanLabeler` wraps a `TransformerEncoder`, the weights of which can be restored from the above pretraining model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B941M4iUCejO"
   },
   "outputs": [],
   "source": [
    "network = modeling.networks.TransformerEncoder(\n",
    "        vocab_size=vocab_size, num_layers=2, sequence_length=sequence_length)\n",
    "\n",
    "# Create a BERT trainer with the created network.\n",
    "bert_span_labeler = modeling.models.BertSpanLabeler(network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QpB9pgj4PpMg"
   },
   "source": [
    "Inspecting the `bert_span_labeler`, we see it wraps the encoder with additional `SpanLabeling` that outputs `start_position` and `end_postion`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RbqRNJCLJu4H"
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(bert_span_labeler, show_shapes=True, dpi=48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fUf1vRxZJwio"
   },
   "outputs": [],
   "source": [
    "# Create a set of 2-dimensional data tensors to feed into the model.\n",
    "word_id_data = np.random.randint(vocab_size, size=(batch_size, sequence_length))\n",
    "mask_data = np.random.randint(2, size=(batch_size, sequence_length))\n",
    "type_id_data = np.random.randint(2, size=(batch_size, sequence_length))\n",
    "\n",
    "# Feed the data to the model.\n",
    "start_logits, end_logits = bert_span_labeler([word_id_data, mask_data, type_id_data])\n",
    "print(start_logits)\n",
    "print(end_logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WqhgQaN1lt-G"
   },
   "source": [
    "### Compute loss\n",
    "With `start_logits` and `end_logits`, we can compute loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "waqs6azNl3Nn"
   },
   "outputs": [],
   "source": [
    "start_positions = np.random.randint(sequence_length, size=(batch_size))\n",
    "end_positions = np.random.randint(sequence_length, size=(batch_size))\n",
    "\n",
    "start_loss = tf.keras.losses.sparse_categorical_crossentropy(\n",
    "    start_positions, start_logits, from_logits=True)\n",
    "end_loss = tf.keras.losses.sparse_categorical_crossentropy(\n",
    "    end_positions, end_logits, from_logits=True)\n",
    "\n",
    "total_loss = (tf.reduce_mean(start_loss) + tf.reduce_mean(end_loss)) / 2\n",
    "print(total_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdf03YtZmd_d"
   },
   "source": [
    "With the `loss`, you can optimize the model. Please see [run_squad.py](https://github.com/tensorflow/models/blob/master/official/nlp/bert/run_squad.py) for the full example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0A1XnGSTChg9"
   },
   "source": [
    "## Classification model\n",
    "\n",
    "In the last section, we show how to build a text classification model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MSK8OpZgnQa9"
   },
   "source": [
    "### Build a BertClassifier model wrapping TransformerEncoder\n",
    "\n",
    "[BertClassifier](https://github.com/tensorflow/models/blob/master/official/nlp/modeling/models/bert_classifier.py) implements a [CLS] token classification model containing a single classification head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cXXCsffkCphk"
   },
   "outputs": [],
   "source": [
    "network = modeling.networks.TransformerEncoder(\n",
    "        vocab_size=vocab_size, num_layers=2, sequence_length=sequence_length)\n",
    "\n",
    "# Create a BERT trainer with the created network.\n",
    "num_classes = 2\n",
    "bert_classifier = modeling.models.BertClassifier(\n",
    "    network, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tZKueKYP4bB"
   },
   "source": [
    "Inspecting the `bert_classifier`, we see it wraps the `encoder` with additional `Classification` head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "snlutm9ZJgEZ"
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(bert_classifier, show_shapes=True, dpi=48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yyHPHsqBJkCz"
   },
   "outputs": [],
   "source": [
    "# Create a set of 2-dimensional data tensors to feed into the model.\n",
    "word_id_data = np.random.randint(vocab_size, size=(batch_size, sequence_length))\n",
    "mask_data = np.random.randint(2, size=(batch_size, sequence_length))\n",
    "type_id_data = np.random.randint(2, size=(batch_size, sequence_length))\n",
    "\n",
    "# Feed the data to the model.\n",
    "logits = bert_classifier([word_id_data, mask_data, type_id_data])\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w--a2mg4nzKm"
   },
   "source": [
    "### Compute loss\n",
    "\n",
    "With `logits`, we can compute `loss`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9X0S1DoFn_5Q"
   },
   "outputs": [],
   "source": [
    "labels = np.random.randint(num_classes, size=(batch_size))\n",
    "\n",
    "loss = modeling.losses.weighted_sparse_categorical_crossentropy_loss(\n",
    "    labels=labels, predictions=tf.nn.log_softmax(logits, axis=-1))\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mzBqOylZo3og"
   },
   "source": [
    "With the `loss`, you can optimize the model. Please see [run_classifier.py](https://github.com/tensorflow/models/blob/master/official/nlp/bert/run_classifier.py) or the colab [fine_tuning_bert.ipynb](https://github.com/tensorflow/models/blob/master/official/colab/fine_tuning_bert.ipynb) for the full example."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Introduction to the TensorFlow Models NLP library",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

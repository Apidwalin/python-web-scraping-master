{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](../images/colab-badge.svg)](https://colab.research.google.com/github/MonashDataFluency/python-web-scraping/blob/master/notebooks/section-3-API-based-scraping.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/api.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A brief introduction to APIs\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will take a look at an alternative way to gather data than the previous pattern based HTML scraping. Sometimes websites offer an API (or Application Programming Interface) as a service which provides a high level interface to directly retrieve data from their repositories or databases at the backend. \n",
    "\n",
    "From Wikipedia,\n",
    "\n",
    "> \"*An API is typically defined as a set of specifications, such as Hypertext Transfer Protocol (HTTP) request messages, along with a definition of the structure of response messages, usually in an Extensible Markup Language (XML) or JavaScript Object Notation (JSON) format.*\"\n",
    "\n",
    "They typically tend to be URL endpoints (to be fired as requests) that need to be modified based on our requirements (what we desire in the response body) which then returns some a payload (data) within the response, formatted as either JSON, XML or HTML. \n",
    "\n",
    "A popular web architecture style called `REST` (or representational state transfer) allows users to interact with web services via `GET` and `POST` calls (two most commonly used) which we briefly saw in the previous section.\n",
    "\n",
    "For example, Twitter's REST API allows developers to access core Twitter data and the Search API provides methods for developers to interact with Twitter Search and trends data.\n",
    "\n",
    "There are primarily two ways to use APIs :\n",
    "\n",
    "- Through the command terminal using URL endpoints, or\n",
    "- Through programming language specific *wrappers*\n",
    "\n",
    "For example, `Tweepy` is a famous python wrapper for Twitter API whereas `twurl` is a command line interface (CLI) tool but both can achieve the same outcomes.\n",
    "\n",
    "Here we focus on the latter approach and will use a Python library (a wrapper) called `wptools` based around the original MediaWiki API.\n",
    "\n",
    "One advantage of using official APIs is that they are usually compliant of the terms of service (ToS) of a particular service that researchers are looking to gather data from. However, third-party libraries or packages which claim to provide more throughput than the official APIs (rate limits, number of requests/sec) generally operate in a gray area as they tend to violate ToS. Always be sure to read their documentation throughly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wikipedia API\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we want to gather some additional data about the Fortune 500 companies and since wikipedia is a rich source for data we decide to use the MediaWiki API to scrape this data. One very good place to start would be to look at the **infoboxes** (as wikipedia defines them) of articles corresponsing to each company on the list. They essentially contain a wealth of metadata about a particular entity the article belongs to which in our case is a company. \n",
    "\n",
    "For e.g. consider the wikipedia article for **Walmart** (https://en.wikipedia.org/wiki/Walmart) which includes the following infobox :\n",
    "\n",
    "![An infobox](../images/infobox.png)\n",
    "\n",
    "As we can see from above, the infoboxes could provide us with a lot of valuable information such as :\n",
    "\n",
    "- Year of founding \n",
    "- Industry\n",
    "- Founder(s)\n",
    "- Products\t\n",
    "- Services\t\n",
    "- Operating income\n",
    "- Net income\n",
    "- Total assets\n",
    "- Total equity\n",
    "- Number of employees etc\n",
    "\n",
    "Although we expect this data to be fairly organized, it would require some post-processing which we will tackle in our next section. We pick a subset of our data and focus only on the top **20** of the Fortune 500 from the full list. \n",
    "\n",
    "Let's begin by installing some of libraries we will use for this excercise as follows,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wptools in c:\\users\\apidwalin\\anaconda3\\envs\\tensorflow1\\lib\\site-packages (0.4.17)\n",
      "Requirement already satisfied: certifi in c:\\users\\apidwalin\\anaconda3\\envs\\tensorflow1\\lib\\site-packages (from wptools) (2020.6.20)\n",
      "Requirement already satisfied: pycurl in c:\\users\\apidwalin\\anaconda3\\envs\\tensorflow1\\lib\\site-packages (from wptools) (7.43.0.5)\n",
      "Requirement already satisfied: html2text in c:\\users\\apidwalin\\anaconda3\\envs\\tensorflow1\\lib\\site-packages (from wptools) (2020.1.16)\n",
      "Requirement already satisfied: lxml in c:\\users\\apidwalin\\anaconda3\\envs\\tensorflow1\\lib\\site-packages (from wptools) (4.5.0)\n",
      "Requirement already satisfied: wikipedia in c:\\users\\apidwalin\\anaconda3\\envs\\tensorflow1\\lib\\site-packages (1.4.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\apidwalin\\anaconda3\\envs\\tensorflow1\\lib\\site-packages (from wikipedia) (4.8.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in c:\\users\\apidwalin\\anaconda3\\envs\\tensorflow1\\lib\\site-packages (from wikipedia) (2.24.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\apidwalin\\anaconda3\\envs\\tensorflow1\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\apidwalin\\anaconda3\\envs\\tensorflow1\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\apidwalin\\anaconda3\\envs\\tensorflow1\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\apidwalin\\anaconda3\\envs\\tensorflow1\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.0.4)\n",
      "Requirement already satisfied: soupsieve>=1.2 in c:\\users\\apidwalin\\anaconda3\\envs\\tensorflow1\\lib\\site-packages (from beautifulsoup4->wikipedia) (1.9.5)\n",
      "Requirement already satisfied: wordcloud in c:\\users\\apidwalin\\anaconda3\\envs\\tensorflow1\\lib\\site-packages (1.6.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\apidwalin\\anaconda3\\envs\\tensorflow1\\lib\\site-packages (from wordcloud) (3.1.3)\n",
      "Requirement already satisfied: pillow in c:\\users\\apidwalin\\anaconda3\\envs\\tensorflow1\\lib\\site-packages (from wordcloud) (8.3.1)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\apidwalin\\anaconda3\\envs\\tensorflow1\\lib\\site-packages (from wordcloud) (1.18.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\apidwalin\\anaconda3\\envs\\tensorflow1\\lib\\site-packages (from matplotlib->wordcloud) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\apidwalin\\anaconda3\\envs\\tensorflow1\\lib\\site-packages (from matplotlib->wordcloud) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\apidwalin\\anaconda3\\envs\\tensorflow1\\lib\\site-packages (from matplotlib->wordcloud) (2.4.6)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\apidwalin\\anaconda3\\envs\\tensorflow1\\lib\\site-packages (from matplotlib->wordcloud) (0.10.0)\n",
      "Requirement already satisfied: six in c:\\users\\apidwalin\\anaconda3\\envs\\tensorflow1\\lib\\site-packages (from cycler>=0.10->matplotlib->wordcloud) (1.14.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\apidwalin\\anaconda3\\envs\\tensorflow1\\lib\\site-packages (from kiwisolver>=1.0.1->matplotlib->wordcloud) (57.2.0)\n"
     ]
    }
   ],
   "source": [
    "# sudo apt install libcurl4-openssl-dev libssl-dev\n",
    "!pip install wptools\n",
    "!pip install wikipedia\n",
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the same,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wptools version : 0.4.17\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import wptools\n",
    "import wikipedia\n",
    "import pandas as pd\n",
    "\n",
    "print('wptools version : {}'.format(wptools.__version__)) # checking the installed version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load the data which we scrapped in the previous section as follows,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fortune_500_companies.csv', <http.client.HTTPMessage at 0x1e497b5dd48>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you dont have the file, you can use the below code to fetch it:\n",
    "import urllib.request\n",
    "url = 'https://raw.githubusercontent.com/MonashDataFluency/python-web-scraping/master/data/fortune_500_companies.csv'\n",
    "urllib.request.urlretrieve(url, 'fortune_500_companies.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>company_name</th>\n",
       "      <th>company_website</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Walmart</td>\n",
       "      <td>http://www.stock.walmart.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Exxon Mobil</td>\n",
       "      <td>http://www.exxonmobil.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Berkshire Hathaway</td>\n",
       "      <td>http://www.berkshirehathaway.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Apple</td>\n",
       "      <td>http://www.apple.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>UnitedHealth Group</td>\n",
       "      <td>http://www.unitedhealthgroup.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rank        company_name                   company_website\n",
       "0     1             Walmart      http://www.stock.walmart.com\n",
       "1     2         Exxon Mobil         http://www.exxonmobil.com\n",
       "2     3  Berkshire Hathaway  http://www.berkshirehathaway.com\n",
       "3     4               Apple              http://www.apple.com\n",
       "4     5  UnitedHealth Group  http://www.unitedhealthgroup.com"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fname = 'fortune_500_companies.csv' # scrapped data from previous section\n",
    "df = pd.read_csv(fname)             # reading the csv file as a pandas df\n",
    "df.head()                           # displaying the first 5 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|    |   rank | company_name       | company_website                  |\n",
    "|---:|-------:|:-------------------|:---------------------------------|\n",
    "|  0 |      1 | Walmart            | http://www.stock.walmart.com     |\n",
    "|  1 |      2 | Exxon Mobil        | http://www.exxonmobil.com        |\n",
    "|  2 |      3 | Berkshire Hathaway | http://www.berkshirehathaway.com |\n",
    "|  3 |      4 | Apple              | http://www.apple.com             |\n",
    "|  4 |      5 | UnitedHealth Group | http://www.unitedhealthgroup.com |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's focus and select only the top 20 companies from the list as follows,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_companies = 20                         # no of companies we are interested \n",
    "df_sub = df.iloc[:no_of_companies, :].copy() # only selecting the top 20 companies\n",
    "companies = df_sub['company_name'].tolist()  # converting the column to a list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a brief look at the same,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Walmart\n",
      "2. Exxon Mobil\n",
      "3. Berkshire Hathaway\n",
      "4. Apple\n",
      "5. UnitedHealth Group\n",
      "6. McKesson\n",
      "7. CVS Health\n",
      "8. Amazon.com\n",
      "9. AT&T\n",
      "10. General Motors\n",
      "11. Ford Motor\n",
      "12. AmerisourceBergen\n",
      "13. Chevron\n",
      "14. Cardinal Health\n",
      "15. Costco\n",
      "16. Verizon\n",
      "17. Kroger\n",
      "18. General Electric\n",
      "19. Walgreens Boots Alliance\n",
      "20. JPMorgan Chase\n"
     ]
    }
   ],
   "source": [
    "for i, j in enumerate(companies):   # looping through the list of 20 company \n",
    "    print('{}. {}'.format(i+1, j))  # printing out the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting article names from wiki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right off the bat, as you might have guessed, one issue with matching the top 20 Fortune 500 companies to their wikipedia article names is that both of them would not be exactly the same i.e. they match character for character. There will be slight variation in their names.\n",
    "\n",
    "To overcome this problem and ensure that we have all the company names and its corresponding wikipedia article, we will use the `wikipedia` package to get suggestions for the company names and their equivalent in wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_search = [{company : wikipedia.search(company)} for company in companies]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting the same,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Walmart :\n",
      "Walmart, Criticism of Walmart, History of Walmart, Walmart (disambiguation), List of Walmart brands, Asda, Walmart Canada, Walmarting, People of Walmart, List of assets owned by Walmart\n",
      "\n",
      "\n",
      "2. Exxon Mobil :\n",
      "ExxonMobil, Exxon, ExxonMobil climate change controversy, Mobil, Exxon Valdez oil spill, ExxonMobil Australia, ExxonMobil Building, Accusations of ExxonMobil human rights violations in Indonesia, Rex Tillerson, Mobil 1\n",
      "\n",
      "\n",
      "3. Berkshire Hathaway :\n",
      "Berkshire Hathaway, List of assets owned by Berkshire Hathaway, Berkshire Hathaway Energy, Warren Buffett, List of Berkshire Hathaway publications, Berkshire Hathaway Assurance, Berkshire Hathaway GUARD Insurance Companies, The World's Billionaires, David L. Sokol, Brooks Sports\n",
      "\n",
      "\n",
      "4. Apple :\n",
      "Apple, Apple Inc., Apple (disambiguation), IPhone, MacOS, IOS, A for Apple, Apple silicon, Apples to Apples, Apple TV\n",
      "\n",
      "\n",
      "5. UnitedHealth Group :\n",
      "UnitedHealth Group, Optum, Pharmacy benefit management, Andrew Witty, Stephen J. Hemsley, Dow Jones Industrial Average, List of largest companies by revenue, William W. McGuire, PacifiCare Health Systems, Health Net\n",
      "\n",
      "\n",
      "6. McKesson :\n",
      "McKesson Corporation, McKesson Europe, DeRay Mckesson, McKesson & Robbins scandal (1938), McKesson (disambiguation), Rexall (Canada), McKesson Plaza, Phillip Musica, LloydsPharmacy, Precious McKesson\n",
      "\n",
      "\n",
      "7. CVS Health :\n",
      "CVS Health, CVS Pharmacy, CVS Caremark, Pharmacy benefit management, CVS, CVS Health Charity Classic, Health Net, Larry Merlo, UnitedHealth Group, America First Policies\n",
      "\n",
      "\n",
      "8. Amazon.com :\n",
      "Amazon (company), Amazon Prime Video, History of Amazon, Criticism of Amazon, List of Amazon products and services, Amazon Appstore, Amazon S3, List of mergers and acquisitions by Amazon, Amazon Prime, Amazon.ae\n",
      "\n",
      "\n",
      "9. AT&T :\n",
      "AT&T, T, AT&T Mobility, AT&T Corporation, AT&T U-verse, T-Pain, T.A.T.u., Ford Model T, T-Series (company), Northrop T-38 Talon\n",
      "\n",
      "\n",
      "10. General Motors :\n",
      "General Motors, General Motors India, History of General Motors, General Motors Vortec engine, List of General Motors factories, General Motors Chapter 11 reorganization, Vauxhall Motors, GMC (automobile), General Motors Canada, General Motors EV1\n",
      "\n",
      "\n",
      "11. Ford Motor :\n",
      "Ford Motor Company, Ford Trimotor, Lincoln Motor Company, Henry Ford, History of Ford Motor Company, Ford of Britain, Ford Germany, Henry Ford II, List of leaders of Ford Motor Company, Edsel Ford\n",
      "\n",
      "\n",
      "12. AmerisourceBergen :\n",
      "AmerisourceBergen, List of largest companies by revenue, Cardinal Health, Ornella Barra, Steven H. Collis, PharMerica, List of S&P 500 companies, The Vanguard Group, Remdesivir, MWI Veterinary Supply\n",
      "\n",
      "\n",
      "13. Chevron :\n",
      "Chevron Corporation, Chevron, Chevron (insignia), Chevron Cars, Chevron Renaissance, Chevron Cars Ltd, Chevron House, Philip Chevron, Wound Chevron, Steven Donziger\n",
      "\n",
      "\n",
      "14. Cardinal Health :\n",
      "Cardinal Health, Cardinal, Catalent, Cordis (medical), Pyxis Corporation, Robert D. Walter, Northern cardinal, The Crime of the Century (2021 film), List of largest companies by revenue, S&P 500 Dividend Aristocrats\n",
      "\n",
      "\n",
      "15. Costco :\n",
      "Costco, American Express, Rotisserie chicken, Costco bear, W. Craig Jelinek, Price Club, Richard Chang (Costco), James Sinegal, Warehouse club, Jeffrey Brotman\n",
      "\n",
      "\n",
      "16. Verizon :\n",
      "Verizon Communications, Verizon Media, Verizon Fios, Verizon (mobile network), Verizon Business, Yahoo!, Verizon Building, Verizon Center, Verizon North, Capital One Arena\n",
      "\n",
      "\n",
      "17. Kroger :\n",
      "Kroger, Murder Kroger, Bernard Kroger, Kroger (disambiguation), Michael Kroger, Uwe Kröger, Kroeger, Chad Kroeger, Kroger Field, Stanley Kamel\n",
      "\n",
      "\n",
      "18. General Electric :\n",
      "General Electric, General Electric Company, General Electric GEnx, General Electric F404, General Electric CF6, General Electric F110, General Electric J85, General Electric Theater, General Electric T700, General Electric LM2500\n",
      "\n",
      "\n",
      "19. Walgreens Boots Alliance :\n",
      "Walgreens Boots Alliance, Alliance Boots, Walgreens, Boots (company), Alliance Healthcare, Rosalind Brewer, Stefano Pessina, Boots Opticians, Deerfield, Illinois, Duane Reade\n",
      "\n",
      "\n",
      "20. JPMorgan Chase :\n",
      "JPMorgan Chase, Chase Bank, JPMorgan Chase Tower (Houston), 2012 JPMorgan Chase trading loss, 270 Park Avenue, 2014 JPMorgan Chase data breach, J.P. Morgan & Co., List of largest banks, Jamie Dimon, JPMorgan Chase Building (Houston)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, company in enumerate(wiki_search):\n",
    "    for i, j in company.items():\n",
    "        print('{}. {} :\\n{}'.format(idx+1, i ,', '.join(j)))\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get the most probable ones (the first suggestion) for each of the first 20 companies on the Fortune 500 list,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Walmart', 'Walmart'), ('Exxon Mobil', 'ExxonMobil'), ('Berkshire Hathaway', 'Berkshire Hathaway'), ('Apple', 'Apple'), ('UnitedHealth Group', 'UnitedHealth Group'), ('McKesson', 'McKesson Corporation'), ('CVS Health', 'CVS Health'), ('Amazon.com', 'Amazon (company)'), ('AT&T', 'AT&T'), ('General Motors', 'General Motors'), ('Ford Motor', 'Ford Motor Company'), ('AmerisourceBergen', 'AmerisourceBergen'), ('Chevron', 'Chevron Corporation'), ('Cardinal Health', 'Cardinal Health'), ('Costco', 'Costco'), ('Verizon', 'Verizon Communications'), ('Kroger', 'Kroger'), ('General Electric', 'General Electric'), ('Walgreens Boots Alliance', 'Walgreens Boots Alliance'), ('JPMorgan Chase', 'JPMorgan Chase')]\n"
     ]
    }
   ],
   "source": [
    "most_probable = [(company, wiki_search[i][company][0]) for i, company in enumerate(companies)]\n",
    "companies = [x[1] for x in most_probable]\n",
    "\n",
    "print(most_probable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice that most of the wiki article titles make sense. However, **Apple** is quite ambiguous in this regard as it can indicate the fruit as well as the company. However we can see that the second suggestion returned by was **Apple Inc.**. Hence, we can manually replace it with **Apple Inc.** as follows,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Walmart', 'ExxonMobil', 'Berkshire Hathaway', 'Apple Inc.', 'UnitedHealth Group', 'McKesson Corporation', 'CVS Health', 'Amazon (company)', 'AT&T', 'General Motors', 'Ford Motor Company', 'AmerisourceBergen', 'Chevron Corporation', 'Cardinal Health', 'Costco', 'Verizon Communications', 'Kroger', 'General Electric', 'Walgreens Boots Alliance', 'JPMorgan Chase']\n"
     ]
    }
   ],
   "source": [
    "companies[companies.index('Apple')] = 'Apple Inc.' # replacing \"Apple\"\n",
    "print(companies) # final list of wikipedia article titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving the infoboxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have mapped the names of the companies to their corresponding wikipedia article let's retrieve the infobox data from those pages. \n",
    "\n",
    "`wptools` provides easy to use methods to directly call the MediaWiki API on our behalf and get us all the wikipedia data. Let's try retrieving data for **Walmart** as follows,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "en.wikipedia.org (parse) Walmart\n",
      "en.wikipedia.org (imageinfo) File:Walmart Home Office.jpg\n",
      "Walmart (en) data\n",
      "{\n",
      "  image: <list(1)> {'kind': 'parse-image', 'file': 'File:Walmart H...\n",
      "  infobox: <dict(30)> name, logo, logo_caption, image, image_size,...\n",
      "  iwlinks: <list(2)> https://commons.wikimedia.org/wiki/Category:W...\n",
      "  pageid: 33589\n",
      "  parsetree: <str(351698)> <root><template><title>short descriptio...\n",
      "  requests: <list(2)> parse, imageinfo\n",
      "  title: Walmart\n",
      "  wikibase: Q483551\n",
      "  wikidata_url: https://www.wikidata.org/wiki/Q483551\n",
      "  wikitext: <str(283370)> {{short description|American multination...\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<wptools.page.WPToolsPage at 0x1e497c01408>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page = wptools.page('Walmart')\n",
    "page.get_parse()    # parses the wikipedia article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the output above, `wptools` successfully retrieved the wikipedia and wikidata corresponding to the query **Walmart**. Now inspecting the fetched attributes,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['requests', 'iwlinks', 'pageid', 'wikitext', 'parsetree', 'infobox', 'title', 'wikibase', 'wikidata_url', 'image'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page.data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attribute **infobox** contains the data we require,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Walmart Inc.',\n",
       " 'logo': 'Walmart logo.svg',\n",
       " 'logo_caption': \"Walmart's current logo since 2008\",\n",
       " 'image': 'Walmart Home Office.jpg',\n",
       " 'image_size': '270px',\n",
       " 'image_caption': 'Walmart Home Office (headquarters)',\n",
       " 'former_name': '{{Unbulleted list|Wal-Mart Discount City (1962–1969)|Wal-Mart, Inc. (1969–1970)|Wal-Mart Stores, Inc. (1970–current)}}',\n",
       " 'type': '[[Public company|Public]]',\n",
       " 'ISIN': 'US9311421039',\n",
       " 'industry': '[[Retail]]',\n",
       " 'traded_as': '{{Unbulleted list|NYSE|WMT|[[DJIA]] component|[[S&P 100]] component|[[S&P 500]] component}} {{NYSE|WMT}}',\n",
       " 'foundation': '{{plainlist|\\n*|Start date and age|1962|7|2| in [[Rogers, Arkansas]]|ref|{{cite web|title=Our History|url=https://corporate.walmart.com/our-story/our-history|website=Corporate.Walmart.com|access-date=July 30, 2020}}|</ref>|\\n* |Start date and age|1969|10|31| in [[Wilmington, Delaware]] (incorporation)}} {{Start date and age|1962|7|2}} in [[Rogers, Arkansas]] * {{Start date and age|1969|10|31}} in [[Wilmington, Delaware]] (incorporation)',\n",
       " 'founder': '[[Sam Walton]]',\n",
       " 'location_city': '[[Bentonville, Arkansas]]',\n",
       " 'location_country': 'U.S.',\n",
       " 'locations': '{{decrease}} 10,526 stores worldwide (April 30, 2021)',\n",
       " 'area_served': 'Worldwide',\n",
       " 'key_people': '{{plainlist|\\n* [[Greg Penner]] ([[Chairman]])\\n* [[Doug McMillon]] ([[President (corporate title)|President]], [[CEO]])}}',\n",
       " 'products': '{{hlist|Electronics|Movies, music, and games|Home and furniture|Home improvement|Clothing|Footwear|Jewelry|Toys|Garden supplies|Health and beauty|Pet supplies|Sporting goods and fitness|Auto|Photo finishing|Craft and party supplies|Grocery}}',\n",
       " 'services': '{{hlist|[[Ria Money Transfer|Walmart-2-Walmart]]|Walmart MoneyCard|Pickup Today|Walmart.com|Financial Services| Walmart Pay}}',\n",
       " 'revenue': '{{increase}} {{US$|559.2 billion|link|=|yes}} (2021)',\n",
       " 'operating_income': '{{increase}} {{US$|22.55 billion}} (2021)',\n",
       " 'net_income': '{{increase}} {{US$|13.70 billion}} (2021)',\n",
       " 'assets': '{{increase}} {{US$|252.5 billion}} (2021)',\n",
       " 'equity': '{{increase}} {{US$|87.53 billion}} (2021)',\n",
       " 'owner': '[[Walton family]] (50.85%)',\n",
       " 'num_employees': '{{plainlist|\\n* 2.2&nbsp;million, Worldwide (2020)|ref| name=\"xbrlus_1\"|{{cite web|url= https://s2.q4cdn.com/056532643/files/doc_financials/2020/q4/Earnings-Release-1.31.2020-Final.pdf}}|</ref>|\\n* 1.5&nbsp;million, U.S. (2017)|ref| name=\"Walmart\"|{{cite web |url = http://corporate.walmart.com/our-story/locations/united-states |title = Walmart Locations Around the World – United States |url-status=live |archive-url = https://web.archive.org/web/20150926012456/http://corporate.walmart.com/our-story/locations/united-states |archive-date = September 26, 2015}}|</ref>|\\n* 700,000, International}} * 1.5&nbsp;million, U.S. (2017) * 700,000, International',\n",
       " 'divisions': \"{{Unbulleted list|Walmart U.S.|Walmart International|[[Sam's Club]]|Global eCommerce}}\",\n",
       " 'subsid': '[[List of assets owned by Walmart|List of subsidiaries]]',\n",
       " 'homepage': '{{URL|walmart.com}}'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page.data['infobox']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a list of features that we want from the infoboxes as follows,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_data = []\n",
    "# attributes of interest contained within the wiki infoboxes\n",
    "features = ['founder', 'location_country', 'revenue', 'operating_income', 'net_income', 'assets',\n",
    "        'equity', 'type', 'industry', 'products', 'num_employees']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now fetching the data for all the companies (this may take a while),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "en.wikipedia.org (parse) Walmart\n",
      "en.wikipedia.org (imageinfo) File:Walmart Home Office.jpg\n",
      "Walmart (en) data\n",
      "{\n",
      "  image: <list(1)> {'kind': 'parse-image', 'file': 'File:Walmart H...\n",
      "  infobox: <dict(30)> name, logo, logo_caption, image, image_size,...\n",
      "  iwlinks: <list(2)> https://commons.wikimedia.org/wiki/Category:W...\n",
      "  pageid: 33589\n",
      "  parsetree: <str(351698)> <root><template><title>short descriptio...\n",
      "  requests: <list(2)> parse, imageinfo\n",
      "  title: Walmart\n",
      "  wikibase: Q483551\n",
      "  wikidata_url: https://www.wikidata.org/wiki/Q483551\n",
      "  wikitext: <str(283370)> {{short description|American multination...\n",
      "}\n",
      "en.wikipedia.org (parse) ExxonMobil\n",
      "ExxonMobil (en) data\n",
      "{\n",
      "  infobox: <dict(24)> name, logo, logo_size, type, traded_as, ISIN...\n",
      "  iwlinks: <list(5)> https://commons.wikimedia.org/wiki/Category:E...\n",
      "  pageid: 18848197\n",
      "  parsetree: <str(195244)> <root><template><title>Short descriptio...\n",
      "  requests: <list(1)> parse\n",
      "  title: ExxonMobil\n",
      "  wikibase: Q156238\n",
      "  wikidata_url: https://www.wikidata.org/wiki/Q156238\n",
      "  wikitext: <str(160770)> {{Short description|American multination...\n",
      "}\n",
      "en.wikipedia.org (parse) Berkshire Hathaway\n",
      "en.wikipedia.org (imageinfo) File:OmahaKiewitPlaza.jpg\n",
      "Berkshire Hathaway (en) data\n",
      "{\n",
      "  image: <list(1)> {'kind': 'parse-image', 'file': 'File:OmahaKiew...\n",
      "  infobox: <dict(26)> name, former_name, logo, logo_size, image, i...\n",
      "  iwlinks: <list(1)> https://commons.wikimedia.org/wiki/Category:B...\n",
      "  pageid: 314333\n",
      "  parsetree: <str(110613)> <root><template><title>short descriptio...\n",
      "  requests: <list(2)> parse, imageinfo\n",
      "  title: Berkshire Hathaway\n",
      "  wikibase: Q217583\n",
      "  wikidata_url: https://www.wikidata.org/wiki/Q217583\n",
      "  wikitext: <str(94041)> {{short description|American multinationa...\n",
      "}\n",
      "en.wikipedia.org (parse) Apple Inc.\n",
      "en.wikipedia.org (imageinfo) File:Apple park cupertino 2019.jpg\n",
      "Apple Inc. (en) data\n",
      "{\n",
      "  image: <list(1)> {'kind': 'parse-image', 'file': 'File:Apple par...\n",
      "  infobox: <dict(35)> name, logo, logo_size, image, image_size, im...\n",
      "  iwlinks: <list(9)> https://commons.wikimedia.org/wiki/Apple_Inc....\n",
      "  pageid: 856\n",
      "  parsetree: <str(437121)> <root><template><title>Redirect</title>...\n",
      "  requests: <list(2)> parse, imageinfo\n",
      "  title: Apple Inc.\n",
      "  wikibase: Q312\n",
      "  wikidata_url: https://www.wikidata.org/wiki/Q312\n",
      "  wikitext: <str(351835)> {{Redirect|Apple (company)|other compani...\n",
      "}\n",
      "en.wikipedia.org (parse) UnitedHealth Group\n",
      "UnitedHealth Group (en) data\n",
      "{\n",
      "  infobox: <dict(19)> name, logo, type, traded_as, founder, key_pe...\n",
      "  pageid: 1845551\n",
      "  parsetree: <str(89052)> <root><template><title>Short description...\n",
      "  requests: <list(1)> parse\n",
      "  title: UnitedHealth Group\n",
      "  wikibase: Q2103926\n",
      "  wikidata_url: https://www.wikidata.org/wiki/Q2103926\n",
      "  wikitext: <str(75354)> {{Short description|American health care ...\n",
      "}\n",
      "en.wikipedia.org (parse) McKesson Corporation\n",
      "McKesson Corporation (en) data\n",
      "{\n",
      "  infobox: <dict(19)> name, logo, type, traded_as, founder, locati...\n",
      "  iwlinks: <list(1)> https://foundation.wikimedia.org/wiki/Terms_of_Use\n",
      "  pageid: 1041603\n",
      "  parsetree: <str(44511)> <root><template><title>Redirect</title><...\n",
      "  requests: <list(1)> parse\n",
      "  title: McKesson Corporation\n",
      "  wikibase: Q570473\n",
      "  wikidata_url: https://www.wikidata.org/wiki/Q570473\n",
      "  wikitext: <str(35385)> {{Redirect|McKesson}}{{short description|...\n",
      "}\n",
      "en.wikipedia.org (parse) CVS Health\n",
      "CVS Health (en) data\n",
      "{\n",
      "  infobox: <dict(30)> name, logo, logo_size, former_name, type, tr...\n",
      "  pageid: 10377597\n",
      "  parsetree: <str(71146)> <root><template><title>about</title><par...\n",
      "  requests: <list(1)> parse\n",
      "  title: CVS Health\n",
      "  wikibase: Q624375\n",
      "  wikidata_url: https://www.wikidata.org/wiki/Q624375\n",
      "  wikitext: <str(56445)> {{about|the parent company previously nam...\n",
      "}\n",
      "en.wikipedia.org (parse) Amazon (company)\n",
      "en.wikipedia.org (imageinfo) File:Amazon Spheres 05.jpg\n",
      "Amazon (company) (en) data\n",
      "{\n",
      "  image: <list(1)> {'kind': 'parse-image', 'file': 'File:Amazon Sp...\n",
      "  infobox: <dict(33)> name, logo, logo_size, logo_caption, image, ...\n",
      "  iwlinks: <list(3)> https://commons.wikimedia.org/wiki/Category:A...\n",
      "  pageid: 90451\n",
      "  parsetree: <str(208612)> <root><template><title>pp</title><part>...\n",
      "  requests: <list(2)> parse, imageinfo\n",
      "  title: Amazon (company)\n",
      "  wikibase: Q3884\n",
      "  wikidata_url: https://www.wikidata.org/wiki/Q3884\n",
      "  wikitext: <str(163616)> {{pp|small=yes}}{{Short description|Amer...\n",
      "}\n",
      "en.wikipedia.org (parse) AT&T\n",
      "en.wikipedia.org (imageinfo) File:AT&THQDallas.jpg\n",
      "AT&T (en) data\n",
      "{\n",
      "  image: <list(1)> {'kind': 'parse-image', 'file': 'File:AT&THQDal...\n",
      "  infobox: <dict(29)> name, logo, logo_caption, logo_size, image, ...\n",
      "  iwlinks: <list(1)> https://commons.wikimedia.org/wiki/Category:AT%26T\n",
      "  pageid: 17555269\n",
      "  parsetree: <str(155118)> <root><template><title>Short descriptio...\n",
      "  requests: <list(2)> parse, imageinfo\n",
      "  title: AT&T\n",
      "  wikibase: Q35476\n",
      "  wikidata_url: https://www.wikidata.org/wiki/Q35476\n",
      "  wikitext: <str(125355)> {{Short description|American multination...\n",
      "}\n",
      "en.wikipedia.org (parse) General Motors\n",
      "en.wikipedia.org (imageinfo) File:RenCen.JPG\n",
      "General Motors (en) data\n",
      "{\n",
      "  image: <list(1)> {'kind': 'parse-image', 'file': 'File:RenCen.JP...\n",
      "  infobox: <dict(31)> name, former_name, logo, logo_size, image, i...\n",
      "  iwlinks: <list(2)> https://commons.wikimedia.org/wiki/Category:G...\n",
      "  pageid: 12102\n",
      "  parsetree: <str(245578)> <root><template><title>short descriptio...\n",
      "  requests: <list(2)> parse, imageinfo\n",
      "  title: General Motors\n",
      "  wikibase: Q81965\n",
      "  wikidata_url: https://www.wikidata.org/wiki/Q81965\n",
      "  wikitext: <str(198605)> {{short description|American automotive ...\n",
      "}\n",
      "en.wikipedia.org (parse) Ford Motor Company\n",
      "en.wikipedia.org (imageinfo) File:FordGlassHouse.jpg\n",
      "Ford Motor Company (en) data\n",
      "{\n",
      "  image: <list(1)> {'kind': 'parse-image', 'file': 'File:FordGlass...\n",
      "  infobox: <dict(28)> name, logo, image, image_size, image_caption...\n",
      "  iwlinks: <list(8)> https://commons.wikimedia.org/wiki/Category:F...\n",
      "  pageid: 30433662\n",
      "  parsetree: <str(233947)> <root><template><title>short descriptio...\n",
      "  requests: <list(2)> parse, imageinfo\n",
      "  title: Ford Motor Company\n",
      "  wikibase: Q44294\n",
      "  wikidata_url: https://www.wikidata.org/wiki/Q44294\n",
      "  wikitext: <str(196686)> {{short description|American multination...\n",
      "}\n",
      "en.wikipedia.org (parse) AmerisourceBergen\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "(28, 'Operation timed out after 300279 milliseconds with 0 out of 0 bytes received')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-f63c42f71bd5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mpage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwptools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompany\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# create a page object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0mpage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_parse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# call the API and parse the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'infobox'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m             \u001b[1;31m# if infobox is present\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\apidwalin\\anaconda3\\envs\\tensorflow1\\lib\\site-packages\\wptools\\page.py\u001b[0m in \u001b[0;36mget_parse\u001b[1;34m(self, show, proxy, timeout)\u001b[0m\n\u001b[0;32m    601\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"get_parse needs title or pageid\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 603\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'parse'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproxy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    604\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\apidwalin\\anaconda3\\envs\\tensorflow1\\lib\\site-packages\\wptools\\core.py\u001b[0m in \u001b[0;36m_get\u001b[1;34m(self, action, show, proxy, timeout)\u001b[0m\n\u001b[0;32m    173\u001b[0m         \u001b[0mqstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_query\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mqobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m         \u001b[0mreq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproxy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mqobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'query'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mqstr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\apidwalin\\anaconda3\\envs\\tensorflow1\\lib\\site-packages\\wptools\\request.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, url, status)\u001b[0m\n\u001b[0;32m     74\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Requests DISABLED\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurl_perform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcrl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcurl_perform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcrl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\apidwalin\\anaconda3\\envs\\tensorflow1\\lib\\site-packages\\wptools\\request.py\u001b[0m in \u001b[0;36mcurl_perform\u001b[1;34m(self, crl)\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[0mbfr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[0mcrl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetopt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcrl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWRITEFUNCTION\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbfr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m         \u001b[0mcrl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mperform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m         \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcurl_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcrl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: (28, 'Operation timed out after 300279 milliseconds with 0 out of 0 bytes received')"
     ]
    }
   ],
   "source": [
    "for company in companies:    \n",
    "    page = wptools.page(company) # create a page object\n",
    "    try:\n",
    "        page.get_parse() # call the API and parse the data\n",
    "        if page.data['infobox'] != None:\n",
    "            # if infobox is present\n",
    "            infobox = page.data['infobox']\n",
    "            # get data for the interested features/attributes\n",
    "            data = { feature : infobox[feature] if feature in infobox else '' \n",
    "                         for feature in features }\n",
    "        else:\n",
    "            data = { feature : '' for feature in features }\n",
    "        \n",
    "        data['company_name'] = company\n",
    "        wiki_data.append(data)\n",
    "        \n",
    "    except KeyError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the first instance in `wiki_data` i.e. **Walmart**,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "wiki_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have successfully retrieved all the infobox data for the companies. Also we can notice that some additional wrangling and cleaning is required which we will perform in the next section. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's export the scraped infoboxes as a single JSON file to a convenient location as follows,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('infoboxes.json', 'w') as file:\n",
    "    json.dump(wiki_data, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "- https://phpenthusiast.com/blog/what-is-rest-api\n",
    "- https://github.com/siznax/wptools/wiki/Data-captured\n",
    "- https://en.wikipedia.org/w/api.php\n",
    "- https://wikipedia.readthedocs.io/en/latest/code.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](../images/colab-badge.svg)](https://colab.research.google.com/github/MonashDataFluency/python-web-scraping/blob/master/notebooks/section-3-API-based-scraping.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/api.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A brief introduction to APIs\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will take a look at an alternative way to gather data than the previous pattern based HTML scraping. Sometimes websites offer an API (or Application Programming Interface) as a service which provides a high level interface to directly retrieve data from their repositories or databases at the backend. \n",
    "\n",
    "From Wikipedia,\n",
    "\n",
    "> \"*An API is typically defined as a set of specifications, such as Hypertext Transfer Protocol (HTTP) request messages, along with a definition of the structure of response messages, usually in an Extensible Markup Language (XML) or JavaScript Object Notation (JSON) format.*\"\n",
    "\n",
    "They typically tend to be URL endpoints (to be fired as requests) that need to be modified based on our requirements (what we desire in the response body) which then returns some a payload (data) within the response, formatted as either JSON, XML or HTML. \n",
    "\n",
    "A popular web architecture style called `REST` (or representational state transfer) allows users to interact with web services via `GET` and `POST` calls (two most commonly used) which we briefly saw in the previous section.\n",
    "\n",
    "For example, Twitter's REST API allows developers to access core Twitter data and the Search API provides methods for developers to interact with Twitter Search and trends data.\n",
    "\n",
    "There are primarily two ways to use APIs :\n",
    "\n",
    "- Through the command terminal using URL endpoints, or\n",
    "- Through programming language specific *wrappers*\n",
    "\n",
    "For example, `Tweepy` is a famous python wrapper for Twitter API whereas `twurl` is a command line interface (CLI) tool but both can achieve the same outcomes.\n",
    "\n",
    "Here we focus on the latter approach and will use a Python library (a wrapper) called `wptools` based around the original MediaWiki API.\n",
    "\n",
    "One advantage of using official APIs is that they are usually compliant of the terms of service (ToS) of a particular service that researchers are looking to gather data from. However, third-party libraries or packages which claim to provide more throughput than the official APIs (rate limits, number of requests/sec) generally operate in a gray area as they tend to violate ToS. Always be sure to read their documentation throughly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wikipedia API\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we want to gather some additional data about the Fortune 500 companies and since wikipedia is a rich source for data we decide to use the MediaWiki API to scrape this data. One very good place to start would be to look at the **infoboxes** (as wikipedia defines them) of articles corresponsing to each company on the list. They essentially contain a wealth of metadata about a particular entity the article belongs to which in our case is a company. \n",
    "\n",
    "For e.g. consider the wikipedia article for **Walmart** (https://en.wikipedia.org/wiki/Walmart) which includes the following infobox :\n",
    "\n",
    "![An infobox](../images/infobox.png)\n",
    "\n",
    "As we can see from above, the infoboxes could provide us with a lot of valuable information such as :\n",
    "\n",
    "- Year of founding \n",
    "- Industry\n",
    "- Founder(s)\n",
    "- Products\t\n",
    "- Services\t\n",
    "- Operating income\n",
    "- Net income\n",
    "- Total assets\n",
    "- Total equity\n",
    "- Number of employees etc\n",
    "\n",
    "Although we expect this data to be fairly organized, it would require some post-processing which we will tackle in our next section. We pick a subset of our data and focus only on the top **20** of the Fortune 500 from the full list. \n",
    "\n",
    "Let's begin by installing some of libraries we will use for this excercise as follows,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wptools\n",
      "  Using cached wptools-0.4.17-py2.py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: certifi in c:\\users\\apidwalin\\anaconda3\\envs\\tf\\lib\\site-packages (from wptools) (2019.11.28)\n",
      "Requirement already satisfied: lxml in c:\\users\\apidwalin\\anaconda3\\envs\\tf\\lib\\site-packages (from wptools) (4.6.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: No metadata found in c:\\users\\apidwalin\\anaconda3\\envs\\tf\\lib\\site-packages\n",
      "ERROR: Could not install packages due to an EnvironmentError: [Errno 2] No such file or directory: 'c:\\\\users\\\\apidwalin\\\\anaconda3\\\\envs\\\\tf\\\\lib\\\\site-packages\\\\lxml-4.6.3.dist-info\\\\METADATA'\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wikipedia in c:\\users\\apidwalin\\anaconda3\\envs\\tf\\lib\\site-packages (1.4.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in c:\\users\\apidwalin\\anaconda3\\envs\\tf\\lib\\site-packages (from wikipedia) (2.24.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\apidwalin\\anaconda3\\envs\\tf\\lib\\site-packages (from wikipedia) (4.8.2)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\apidwalin\\anaconda3\\envs\\tf\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.25.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\apidwalin\\anaconda3\\envs\\tf\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\apidwalin\\anaconda3\\envs\\tf\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2019.11.28)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\apidwalin\\anaconda3\\envs\\tf\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.8)\n",
      "Requirement already satisfied: soupsieve>=1.2 in c:\\users\\apidwalin\\anaconda3\\envs\\tf\\lib\\site-packages (from beautifulsoup4->wikipedia) (1.9.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Error while checking for conflicts. Please file an issue on pip's issue tracker: https://github.com/pypa/pip/issues/new\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Apidwalin\\Anaconda3\\envs\\tf\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 3021, in _dep_map\n",
      "    return self.__dep_map\n",
      "  File \"C:\\Users\\Apidwalin\\Anaconda3\\envs\\tf\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 2815, in __getattr__\n",
      "    raise AttributeError(attr)\n",
      "AttributeError: _DistInfoDistribution__dep_map\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Apidwalin\\Anaconda3\\envs\\tf\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 3012, in _parsed_pkg_info\n",
      "    return self._pkg_info\n",
      "  File \"C:\\Users\\Apidwalin\\Anaconda3\\envs\\tf\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 2815, in __getattr__\n",
      "    raise AttributeError(attr)\n",
      "AttributeError: _pkg_info\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Apidwalin\\Anaconda3\\envs\\tf\\lib\\site-packages\\pip\\_internal\\commands\\install.py\", line 535, in _determine_conflicts\n",
      "    return check_install_conflicts(to_install)\n",
      "  File \"C:\\Users\\Apidwalin\\Anaconda3\\envs\\tf\\lib\\site-packages\\pip\\_internal\\operations\\check.py\", line 108, in check_install_conflicts\n",
      "    package_set, _ = create_package_set_from_installed()\n",
      "  File \"C:\\Users\\Apidwalin\\Anaconda3\\envs\\tf\\lib\\site-packages\\pip\\_internal\\operations\\check.py\", line 50, in create_package_set_from_installed\n",
      "    package_set[name] = PackageDetails(dist.version, dist.requires())\n",
      "  File \"C:\\Users\\Apidwalin\\Anaconda3\\envs\\tf\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 2736, in requires\n",
      "    dm = self._dep_map\n",
      "  File \"C:\\Users\\Apidwalin\\Anaconda3\\envs\\tf\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 3023, in _dep_map\n",
      "    self.__dep_map = self._compute_dependencies()\n",
      "  File \"C:\\Users\\Apidwalin\\Anaconda3\\envs\\tf\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 3032, in _compute_dependencies\n",
      "    for req in self._parsed_pkg_info.get_all('Requires-Dist') or []:\n",
      "  File \"C:\\Users\\Apidwalin\\Anaconda3\\envs\\tf\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 3014, in _parsed_pkg_info\n",
      "    metadata = self.get_metadata(self.PKG_INFO)\n",
      "  File \"C:\\Users\\Apidwalin\\Anaconda3\\envs\\tf\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 1420, in get_metadata\n",
      "    value = self._get(path)\n",
      "  File \"C:\\Users\\Apidwalin\\Anaconda3\\envs\\tf\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 1616, in _get\n",
      "    with open(path, 'rb') as stream:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'c:\\\\users\\\\apidwalin\\\\anaconda3\\\\envs\\\\tf\\\\lib\\\\site-packages\\\\lxml-4.6.3.dist-info\\\\METADATA'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordcloud in c:\\users\\apidwalin\\anaconda3\\envs\\tf\\lib\\site-packages (1.8.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\apidwalin\\anaconda3\\envs\\tf\\lib\\site-packages (from wordcloud) (3.4.2)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\apidwalin\\anaconda3\\envs\\tf\\lib\\site-packages (from wordcloud) (1.18.1)\n",
      "Requirement already satisfied: pillow in c:\\users\\apidwalin\\anaconda3\\envs\\tf\\lib\\site-packages (from wordcloud) (8.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\apidwalin\\anaconda3\\envs\\tf\\lib\\site-packages (from matplotlib->wordcloud) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\apidwalin\\anaconda3\\envs\\tf\\lib\\site-packages (from matplotlib->wordcloud) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\apidwalin\\anaconda3\\envs\\tf\\lib\\site-packages (from matplotlib->wordcloud) (1.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\apidwalin\\anaconda3\\envs\\tf\\lib\\site-packages (from matplotlib->wordcloud) (2.4.7)\n",
      "Requirement already satisfied: six in c:\\users\\apidwalin\\anaconda3\\envs\\tf\\lib\\site-packages (from cycler>=0.10->matplotlib->wordcloud) (1.14.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\apidwalin\\anaconda3\\envs\\tf\\lib\\site-packages (from kiwisolver>=1.0.1->matplotlib->wordcloud) (49.6.0.post20200814)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Error while checking for conflicts. Please file an issue on pip's issue tracker: https://github.com/pypa/pip/issues/new\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Apidwalin\\Anaconda3\\envs\\tf\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 3021, in _dep_map\n",
      "    return self.__dep_map\n",
      "  File \"C:\\Users\\Apidwalin\\Anaconda3\\envs\\tf\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 2815, in __getattr__\n",
      "    raise AttributeError(attr)\n",
      "AttributeError: _DistInfoDistribution__dep_map\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Apidwalin\\Anaconda3\\envs\\tf\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 3012, in _parsed_pkg_info\n",
      "    return self._pkg_info\n",
      "  File \"C:\\Users\\Apidwalin\\Anaconda3\\envs\\tf\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 2815, in __getattr__\n",
      "    raise AttributeError(attr)\n",
      "AttributeError: _pkg_info\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Apidwalin\\Anaconda3\\envs\\tf\\lib\\site-packages\\pip\\_internal\\commands\\install.py\", line 535, in _determine_conflicts\n",
      "    return check_install_conflicts(to_install)\n",
      "  File \"C:\\Users\\Apidwalin\\Anaconda3\\envs\\tf\\lib\\site-packages\\pip\\_internal\\operations\\check.py\", line 108, in check_install_conflicts\n",
      "    package_set, _ = create_package_set_from_installed()\n",
      "  File \"C:\\Users\\Apidwalin\\Anaconda3\\envs\\tf\\lib\\site-packages\\pip\\_internal\\operations\\check.py\", line 50, in create_package_set_from_installed\n",
      "    package_set[name] = PackageDetails(dist.version, dist.requires())\n",
      "  File \"C:\\Users\\Apidwalin\\Anaconda3\\envs\\tf\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 2736, in requires\n",
      "    dm = self._dep_map\n",
      "  File \"C:\\Users\\Apidwalin\\Anaconda3\\envs\\tf\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 3023, in _dep_map\n",
      "    self.__dep_map = self._compute_dependencies()\n",
      "  File \"C:\\Users\\Apidwalin\\Anaconda3\\envs\\tf\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 3032, in _compute_dependencies\n",
      "    for req in self._parsed_pkg_info.get_all('Requires-Dist') or []:\n",
      "  File \"C:\\Users\\Apidwalin\\Anaconda3\\envs\\tf\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 3014, in _parsed_pkg_info\n",
      "    metadata = self.get_metadata(self.PKG_INFO)\n",
      "  File \"C:\\Users\\Apidwalin\\Anaconda3\\envs\\tf\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 1420, in get_metadata\n",
      "    value = self._get(path)\n",
      "  File \"C:\\Users\\Apidwalin\\Anaconda3\\envs\\tf\\lib\\site-packages\\pip\\_vendor\\pkg_resources\\__init__.py\", line 1616, in _get\n",
      "    with open(path, 'rb') as stream:\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'c:\\\\users\\\\apidwalin\\\\anaconda3\\\\envs\\\\tf\\\\lib\\\\site-packages\\\\lxml-4.6.3.dist-info\\\\METADATA'\n"
     ]
    }
   ],
   "source": [
    "# sudo apt install libcurl4-openssl-dev libssl-dev\n",
    "!pip install wptools\n",
    "!pip install wikipedia\n",
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the same,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wptools'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-4b5f107141b9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mwptools\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwikipedia\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wptools'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import wptools\n",
    "import wikipedia\n",
    "import pandas as pd\n",
    "\n",
    "print('wptools version : {}'.format(wptools.__version__)) # checking the installed version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load the data which we scrapped in the previous section as follows,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you dont have the file, you can use the below code to fetch it:\n",
    "import urllib.request\n",
    "url = 'https://raw.githubusercontent.com/MonashDataFluency/python-web-scraping/master/data/fortune_500_companies.csv'\n",
    "urllib.request.urlretrieve(url, 'fortune_500_companies.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fname = 'fortune_500_companies.csv' # scrapped data from previous section\n",
    "df = pd.read_csv(fname)             # reading the csv file as a pandas df\n",
    "df.head()                           # displaying the first 5 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|    |   rank | company_name       | company_website                  |\n",
    "|---:|-------:|:-------------------|:---------------------------------|\n",
    "|  0 |      1 | Walmart            | http://www.stock.walmart.com     |\n",
    "|  1 |      2 | Exxon Mobil        | http://www.exxonmobil.com        |\n",
    "|  2 |      3 | Berkshire Hathaway | http://www.berkshirehathaway.com |\n",
    "|  3 |      4 | Apple              | http://www.apple.com             |\n",
    "|  4 |      5 | UnitedHealth Group | http://www.unitedhealthgroup.com |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's focus and select only the top 20 companies from the list as follows,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_companies = 20                         # no of companies we are interested \n",
    "df_sub = df.iloc[:no_of_companies, :].copy() # only selecting the top 20 companies\n",
    "companies = df_sub['company_name'].tolist()  # converting the column to a list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a brief look at the same,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, j in enumerate(companies):   # looping through the list of 20 company \n",
    "    print('{}. {}'.format(i+1, j))  # printing out the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting article names from wiki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right off the bat, as you might have guessed, one issue with matching the top 20 Fortune 500 companies to their wikipedia article names is that both of them would not be exactly the same i.e. they match character for character. There will be slight variation in their names.\n",
    "\n",
    "To overcome this problem and ensure that we have all the company names and its corresponding wikipedia article, we will use the `wikipedia` package to get suggestions for the company names and their equivalent in wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_search = [{company : wikipedia.search(company)} for company in companies]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting the same,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for idx, company in enumerate(wiki_search):\n",
    "    for i, j in company.items():\n",
    "        print('{}. {} :\\n{}'.format(idx+1, i ,', '.join(j)))\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get the most probable ones (the first suggestion) for each of the first 20 companies on the Fortune 500 list,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_probable = [(company, wiki_search[i][company][0]) for i, company in enumerate(companies)]\n",
    "companies = [x[1] for x in most_probable]\n",
    "\n",
    "print(most_probable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice that most of the wiki article titles make sense. However, **Apple** is quite ambiguous in this regard as it can indicate the fruit as well as the company. However we can see that the second suggestion returned by was **Apple Inc.**. Hence, we can manually replace it with **Apple Inc.** as follows,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies[companies.index('Apple')] = 'Apple Inc.' # replacing \"Apple\"\n",
    "print(companies) # final list of wikipedia article titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving the infoboxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have mapped the names of the companies to their corresponding wikipedia article let's retrieve the infobox data from those pages. \n",
    "\n",
    "`wptools` provides easy to use methods to directly call the MediaWiki API on our behalf and get us all the wikipedia data. Let's try retrieving data for **Walmart** as follows,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "page = wptools.page('Walmart')\n",
    "page.get_parse()    # parses the wikipedia article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the output above, `wptools` successfully retrieved the wikipedia and wikidata corresponding to the query **Walmart**. Now inspecting the fetched attributes,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page.data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attribute **infobox** contains the data we require,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page.data['infobox']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a list of features that we want from the infoboxes as follows,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_data = []\n",
    "# attributes of interest contained within the wiki infoboxes\n",
    "features = ['founder', 'location_country', 'revenue', 'operating_income', 'net_income', 'assets',\n",
    "        'equity', 'type', 'industry', 'products', 'num_employees']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now fetching the data for all the companies (this may take a while),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for company in companies:    \n",
    "    page = wptools.page(company) # create a page object\n",
    "    try:\n",
    "        page.get_parse() # call the API and parse the data\n",
    "        if page.data['infobox'] != None:\n",
    "            # if infobox is present\n",
    "            infobox = page.data['infobox']\n",
    "            # get data for the interested features/attributes\n",
    "            data = { feature : infobox[feature] if feature in infobox else '' \n",
    "                         for feature in features }\n",
    "        else:\n",
    "            data = { feature : '' for feature in features }\n",
    "        \n",
    "        data['company_name'] = company\n",
    "        wiki_data.append(data)\n",
    "        \n",
    "    except KeyError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the first instance in `wiki_data` i.e. **Walmart**,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "wiki_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have successfully retrieved all the infobox data for the companies. Also we can notice that some additional wrangling and cleaning is required which we will perform in the next section. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's export the scraped infoboxes as a single JSON file to a convenient location as follows,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('infoboxes.json', 'w') as file:\n",
    "    json.dump(wiki_data, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "- https://phpenthusiast.com/blog/what-is-rest-api\n",
    "- https://github.com/siznax/wptools/wiki/Data-captured\n",
    "- https://en.wikipedia.org/w/api.php\n",
    "- https://wikipedia.readthedocs.io/en/latest/code.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
